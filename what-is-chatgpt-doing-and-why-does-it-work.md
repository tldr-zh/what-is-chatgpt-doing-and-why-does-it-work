# ChatGPT 如何工作？

原文：[https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work)

本篇文章由 [太长不看汉化组 TLDR-ZH](https://github.com/tldr-zh) 翻译，并进行内容浓缩。

[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)

本篇文章同样适用于当前其他的 **大型语言模型** [Large Language Models, LLM]。

## 每次添加一个词语，如此循环重复

ChatGPT 从根本上一直试图做的是：提前学习大量的文本数据后，对每个用户提供的文本产生「合理的延续」。

假设我们要写一篇文章，开头为：「人工智能最强的地方在于…」。

ChatGPT 会生成一个列表，这个列表包含了接下来最相关词语的概率：

| 词语 | 最相关概率 |
| --- | --- |
| 学习 | 4.5% |
| 预测 | 3.5% |
| 创造 | 3.2% |
| 理解 | 3.1% |

每次添加一个词语后，ChatGPT 会循环重复这个操作，一遍一遍地添加词语。

但是它每次应该选择哪一个词语添加到文章中呢？

如果我们总是选择排名最高的词，通常会得到一篇非常「平庸」的文章，看起来永远不会显示出任何创造力，甚至有时候会逐字重复。

但如果我们偶尔随机的选择排名较低的词语，我们会得到一篇「更有趣」的文章。

这个随机就是所谓的「温度」参数（Temperature），这个参数决定了排名较低的词语被使用的频率（对于生成文章，这个参数的最佳值似乎是 0.8）。

## 概率从何而来

人类所有的文本量太大，企图穷尽所有的句子，并为这些句子构建下一个词语概率列表是不可能的。

我们可以建立一个神经网络模型，用来 **估计** 下一个词发生的概率（注意是估计）。

ChatGPT 的核心正式所谓的「大型语言模型」（LLM），它是为了很好地 **估计** 词语概率而构建的。

## 词嵌入的概念

神经网络是基于数字的，因此，如果我们要使用它们处理文本之类的东西，我们需要一种将文本转化为数字的方法，即：词嵌入（Word embedding）。

在过去 10 年里，人类已经开发出了一系列词嵌入系统：word2vec、GloVe、BERT、GPT 等等。

严格来说，ChatGPT 不是单纯地处理单词，而是处理 Token。Token 可以是完整的单词，也可以是像「pre」、「ing」、「ized」这样的片段。使用 Token 使得 ChatGPT 更加灵活。

## ChatGPT 内部

首先是嵌入模块：将 Token 转变为词嵌入向量，同时还有一条「第二路径」：根据 Token 在文本序列中的位置创建另一个嵌入向量。最后将这两个向量相加，得到嵌入模块的最终向量。

ChatGPT 作为一个特别为处理语言而建立的神经网络，它最显著的特点就是一块被称为「Transformer」的神经网络结构。Transformer 引入了「注意」的概念（Attention），Transformer 的注意力机制所做的事是允许模型在后续能够回忆起先前遇到过的单词，从而捕捉到潜在的联系。

在嵌入模块后，轮到 Transformer 登场：将嵌入向量划分成多个块，每个块由多个「注意头」（Attention heads）单独处理，然后产生新的、加权后的新向量。

ChatGPT 最后的部分就是根据这些向量，生成下一个 Token 应该出现的概率列表。

## ChatGPT 的成功带来的启发

在某种程度上，ChatGPT 的基本概念相当简单，它由非常简单的神经元组成，虽然神经元的数量是数十亿级别的，但是每个神经元本质上就是接收一个数字，进行计算，然后传递给下一个。

人类的语言，一直以来似乎代表着某种复杂性的巅峰。但是今天，像 ChatGPT 这样的东西可以在语言这个领域上走得这么远，我认为背后的基本答案是：人类的语言在某种程度上其实比表面上看起来更简单，以至于 ChatGPT 在训练过程中以某种方式「隐含」地发现了人类语言的规律。我认为：如果我们能让 ChatGPT 运行得更高效、更直接、更透明，那么一个重大的「语言法则」就会被我们发现。
